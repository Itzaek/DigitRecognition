{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 3: February 28th\n",
    "\n",
    "__Logistic Regression__\n",
    "<br></br>\n",
    "The input matrix is a flattened version of the image, made into a [1*784] matrix. The one in the matrix means that it has only one channel. It usually deals with color, but for this set it's specifically grayscale. The matrix is multiplied by a [784*10] matrix yields a resulting matrix of [1,10]. This means that there are 10 outputs because w ewant to test for 10 numbers. For example, if we were testing the number 5, the resulting matrix would be [0,0,0,0,1,0,0,0,0,0].\n",
    "\n",
    "Another important aspect are hidden layers. We have no idea what these layers do, but they do computations until it reaches the output that we want (sort of like intermediate steps). These layers still check what pixels we want and don't want. In order to be able to to compute the result of a matrix, the model must have multiple layers. We use the concept of non linearity to transform the function to make it easier. For example, you can't cut a multi segmented straight line with different colors with one line. When performing non linearity, it gets the output of the hidden layer. Furthermore, as a side note, the transformations one performs on a function is called the activation functions.\n",
    "\n",
    "![Activation Function](https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/img/spiral.1-2.2-2-2-2-2-2.gif)\n",
    "\n",
    "While training your model, it is important to realize the differences between underfitting, desired and overfitting. It is extremely dangerous to have a 99% training model (which could potentially end up being an overfit), but when you test it with an actual set, it can be 10% accurate.\n",
    "    \n",
    "__Computation Graph__\n",
    "<br></br>\n",
    "A computation graph is a cyclic graph. This means that it can go back into itself, so step A can go to step B, but in the same way you can go from step B to step A.\n",
    "\n",
    "** When doing machine learning, it's always important to preprocess your data. Usually you're dealing with large data sets. Even if they don't start very large, the training process includes many matrix multiplications which can lead to numbers that are too large and take up too much memory. **\n",
    "\n",
    "Finally, the model employs a stochastic gradient descent - meaning random. In the stochastic gradient descent, we're trying to find the gradient of the weights associated with the values we are testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
