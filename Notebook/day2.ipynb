{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day2: January 31st \n",
    "<br></br>\n",
    "<u>Introduction to tensorflow</u>\n",
    "<br></br>\n",
    "What is the MNIST data set?\n",
    "\n",
    "There are many different MNIST data sets, but in this project we will be using the MNIST data set of hand written digits. It's a training set of 60,000 hand written digits and a test set of 10,000 hand written digits. Image recognition is one of the basic forms of machine learning application and is similar to the 'Hello World' of machine learning.\n",
    "\n",
    "__             Fig 1. Example of MNIST data of written digits          __\n",
    "\n",
    "![MNIST data set](http://neuralnetworksanddeeplearning.com/images/mnist_100_digits.png)\n",
    "\n",
    "Linear Regression is used to supervised learning on the set of training models. Supervised learning is where you have both the input and the output values and you map the model based on those values (using linear regression). Following this is the equation depicting linear regression on a model and a corresponding graph.\n",
    "\n",
    "$Y_i = \\beta_0 + \\beta_1X_i + \\epsilon_i$\n",
    "\n",
    "![Linear regression graph](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Linear_regression.svg/1200px-Linear_regression.svg.png)\n",
    "\n",
    "In this particular problem, the type of function that will be modelling the model is a softmax function which is a generalization of the sigmoid function.\n",
    "\n",
    "$f(x) = \\frac{1}{1+e^-(x)}$\n",
    "\n",
    "In this part of the code, the tensorflow library has its own softmax function that's already implemented for you to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.nn.softmax(tf.matmul(x,W)+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross entropy loss is another important function that outputs the probability (from 0 to 1) that the prediction probability diverges from the actual value on the label of the image. This is modelled by the function\n",
    "\n",
    "$$CE = - \\sum_{i}^{C} t_i log(f(s)_i)$$\n",
    "$$CE = - \\sum_{i = 1}^{C' = 2} t_i log(f(s_i)) = -t_1 log(f(s_1)) - (1-t_1) log(1-f(s_1))$$\n",
    "\n",
    "In the code, we use the tensorflow library's function to model this equation for the training model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y_, logits = y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the stochastic gradient descent, we're able to train the model until it achieves a high accuracy. The stochastic gradient descent is an algorithm that starts at a random point and on a function and iteratively travels down the function until its slope (so the tangent to the function) reaches 0.\n",
    "\n",
    "This is modelled by the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.5\n",
    "number_of_steps = 10000\n",
    "batch_size = 100\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the model with the training examples provided and the stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(number_of_steps):\n",
    "    batch = mnist.train.next_batch(batch_size)\n",
    "    train_step.run(feed_dict={x: batch[0], y_: batch[1]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And evaluate the accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the model\n",
    "correct_pred = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Print out the accuracy\n",
    "acc_eval = accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels})\n",
    "print(f\"Current accuracy: %{acc_eval * 100}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
